# This is a spider that collects data rapod7 database
import scrapy,os,sys,re
from web_scraping.items import WebScrapingItem

packages = os.popen("dpkg -l | grep '^ii' | awk '{print $2}'").read().split('\n')   #get the packages' name using dpkg
versions = os.popen("dpkg -l | grep '^ii' | awk '{print $3}'").read().split('\n')   #get the packages' version
kernel_version= os.popen('uname -r').read() #get the linux kernel version
kernel_version=kernel_version[:6]

class QuotesSpider(scrapy.Spider):
    results=()
    start_urls=[ 'https://www.rapid7.com/db/?q=linux kernel'+ ' ' + kernel_version]
    for i,j in zip(packages,versions):
        j=j[:3]
        results=(i+" "+(re.sub(r'([^\w\s]|_)+(?=\s|$)', '', j)))
       
        name = "rapid7"
 
        if str(results) != str(' '): 
            start_urls.append('https://www.rapid7.com/db/?q='+str(results))
        
    def parse(self, response):
        item = WebScrapingItem()
        name = response.css('.search-query > input:nth-child(1)::attr(value)').get()
        
        for quote in response.css('section.vulndb__results'):
            item['name'] = quote.css('div.resultblock__info-title::text').get() # save the result into item in order to store it in a better csv format
            vul = re.sub('[\W_]+', ' ', str(item)) #remove special characters
            vul = vul[12:] #remove some bad characters at the start of the string
            vul =  vul[:-4] #remove some bad characters at the end of the string
            item['vulnerability']=vul 
            item ['name'] = name
            yield item
        next_page = response.css('.large-12 > ul:nth-child(1) > li:nth-child(3) > a::attr(href)').get()
  
        if next_page is not None:
            yield response.follow(next_page, callback=self.parse)
       

